Following are the details of the folders available here:

***Note:*** Here, the file iteration-x, if x = 1, 2, 3, 4, 5, or 6: the data set trained and tested without normalization; else if x = 7, 8, 9, 10, 11, or 12: the data set trained and tested with normalization.

**iteration-1 & iteration-7**: Consists of training and testing outputs, where:
- Activation function between the layers = Tanh
- Optimizer = Adam
- Loss Function = SigmoidCrossEntropy
- Learning Rate = 0.001
- Number of iterations = 500
- Number of hidden layers = 2

**iteration-2 & iteration-8**: Consists of training and testing outputs, where:
- Activation function between the layers = Tanh
- Optimizer = Adam
- Loss Function = SigmoidCrossEntropy
- Learning Rate = 0.1 - *(Change)*
- Number of iterations = 500
- Number of hidden layers = 2

**iteration-3 & iteration-9**: Consists of training and testing outputs, where:
- Activation function between the layers = SoftPlus - *(Change)*
- Optimizer = Adam
- Loss Function = SigmoidCrossEntropy
- Learning Rate = 0.001
- Number of iterations = 200
- Number of hidden layers = 2

**iteration-4 & iteration-10**: Consists of training and testing outputs, where:
- Activation function between the layers = Tanh
- Optimizer = SGD - *(Change)*
- Loss Function = SigmoidCrossEntropy
- Learning Rate = 0.001
- Number of iterations = 200
- Number of hidden layers = 2

**iteration-5 & iteration-11**: Consists of training and testing outputs, where:
- Activation function between the layers = Tanh
- Optimizer = Adam
- Loss Function = MSE - *(Change)*
- Learning Rate = 0.001
- Number of iterations = 200
- Number of hidden layers = 2

**iteration-6 & iteration-12**: Consists of training and testing outputs, where:
- Activation function between the layers = Tanh
- Optimizer = Adam
- Loss Function = SigmoidCrossEntropy
- Learning Rate = 0.001
- Number of iterations = 200
- Number of hidden layers = 1 - *(Change)*
